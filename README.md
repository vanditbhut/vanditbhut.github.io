# Vandit Bhut

Welcome to my portfolio! I am a Data Science graduate with a master's degree from Swansea University, specializing in Data analysis, Machine learning, and Data engineering. I am passionate about transforming raw data into actionable insights and leveraging technical expertise to solve complex business challenges. Below is a summary of my skills, experience, and projects. 

---

## About Me
I have experience in collecting, cleaning, and analyzing diverse datasets, developing predictive models, and building data visualizations for data-informed decision-making. I am proficient in Python, SQL, Power BI, and AWS, with hands-on experience in optimizing workflows and implementing ETL pipelines. 

---

## Contact Information
📍 Swansea, United Kingdom | 📞 +44 739-306-5231 | 📧 vanditbhut12@gmail.com | [LinkedIn](https://www.linkedin.com/in/vandit-bhut/){:target="_blank"}

---

## 🎓 Education

**Master of Science in Data Science**  
Swansea University, UK (Sep 2023 - Sep 2024)

**Bachelor of Technology in Information and Communication Technology**  
DA-IICT, India (May 2018 - Jun 2022)

---

## 🛠️ Skills
**Technical Skills**  
- **Programming Languages**: Python, SQL (MySQL, PostgreSQL), C, C++, HTML, CSS, JavaScript (Basic)  
- **Data Tools**: Power BI, AWS (S3, Lambda), MongoDB, Jupyter Notebook, Google Colab, GitHub  
- **Data Engineering**: ETL pipelines, data cleaning, optimization  
- **Frameworks**: Machine Learning (Scikit-learn, TensorFlow), Data Visualization (Matplotlib, Seaborn)  
- **Dev Tools**: VS Code, Git  
- **Core Competencies**: Problem-solving, Data Analysis, Time Management, Attention to Detail, Communication, High level of adaptability, leadership, Team work

---

## 💼 Experience

### Junior Data Acquisition & Curation Scientist  
**Opendatabay, UK** (Oct 2024 - Present)  
- Gathered and organized over 100 publicly available datasets into a structured system, making them easier to find and reuse.
- Created clear and consistent metadata for each dataset, which reduced the time needed to locate specific information.
- Worked closely with developers and analysts to ensure datasets were compatible with internal tools and platforms.

### Web Designer Intern  
**Midnight Digital Pvt. Ltd., India** (Mar 2023 - Jun 2023)  
- Improved loading speed and performance on 10+ client websites by cleaning up code and optimizing assets.
- Delivered web design projects for clients like [ENpower School](https://enpower-school.com/){:target="_blank"} and [IFT](https://indiafuturetycoons.com/){:target="_blank"}, following specific brand and content requirements.
- Supported junior team members by reviewing their work and sharing basic design and coding techniques.

### Data Analytics Job Simulation (Online)  
**Accenture North America** (Jun 2024)  
- Analysed 7 datasets comprising 100,000+ records, uncovering critical business trends that informed stakeholder strategies.
- Developed presentations that simplified data, resulting in 10+ recommendations and a 30% boost in stakeholder engagement.


### Power BI Job Simulation (Online)  
**PwC Switzerland** (Jun 2024)  
- Built and implemented a Power BI dashboard to address HR challenges, improving reporting efficiency by 40%.
- Utilized filters, visuals, and calculated fields with DAX across 3 fields to create an easy-to-use and informative dashboard.
- Delivered comprehensive insights that supported data-driven decision-making and actionable business improvements.

---

## 💻 Projects


## Project: 1 📦 **UK Amazon Consumer Trends Analysis**  

**Date:** December 2024  
**Tools & Technologies:** SQL, Python (Pandas, NumPy, NLTK), Power BI, Jupyter Notebook  

---

#### 🧠 **Project Overview:**  
This project aimed to uncover **key consumer behavior patterns and sales trends** on Amazon’s UK marketplace by analyzing a large-scale dataset of over **1 million product listings, customer reviews, and pricing data**. The goal was to transform raw, unstructured, and noisy data into **actionable insights** that could inform marketing, pricing, and product strategies.

---

#### 👨‍💻 **My Role:**  
As the sole data analyst and engineer on the project, I led the end-to-end data pipeline — from data extraction and cleaning to analysis and dashboard creation — demonstrating full ownership and technical versatility.

---

#### 🛠️ **Key Technical Contributions:**

- **SQL Data Engineering:**
  - Designed and implemented **optimized SQL queries** to efficiently extract structured data from a large dataset.
  - Structured the dataset into **relational tables** based on product category, review metadata, and pricing info to improve accessibility and speed during analysis.
  - Ensured **scalability and performance** by indexing key fields and minimizing query load time.

- **Python-based Data Wrangling & Sentiment Analysis:**
  - Cleaned and standardized messy data using `pandas`, handling missing values, duplicates, and inconsistent text fields.
  - Applied **Natural Language Processing (NLP)** using the `NLTK` library to conduct sentiment analysis on **hundreds of thousands of customer reviews**, classifying them as positive, neutral, or negative.
  - Created custom **visual plots** (e.g., word clouds, polarity score distributions) to showcase sentiment insights across product categories.

- **Interactive Dashboard (Power BI):**
  - Built a **multi-page Power BI dashboard** to visualize:
    - 🛍️ *Sales trends* over time by product category
    - 💸 *Price distribution* across categories and brands
    - 😃 *Customer sentiment patterns* using review data
    - 📈 *Top-performing products* by rating and sales
  - Used **slicers, filters, and tooltips** to make the dashboard fully interactive and user-friendly for non-technical stakeholders.

---

#### 📊 **Impact & Insights:**
- Discovered that **mid-range priced products** in electronics had the highest customer satisfaction, while lower-rated products had higher return complaints.
- Identified **seasonal sales spikes** in December and July, suggesting strong correlation with promotional periods.
- The sentiment analysis revealed **delivery speed** and **packaging quality** as the most frequent negative themes in low-rated reviews.

---

## Project: 2 🎬 **Marvel Movie Universe (MCU) Data Analysis**  
**Date:** December 2024  
**Tools & Technologies:** Python (Pandas, Matplotlib, Seaborn), AWS S3, Jupyter Notebook  

---

### 🧠 **Project Overview:**  
This project explored the evolution of the Marvel Cinematic Universe (MCU) through a **data-driven lens**. Using Python and cloud-hosted data from **Amazon S3**, I analyzed over 30 MCU movies to uncover **release trends**, **box office patterns**, and **character appearances** over time. The objective was to transform raw movie data into **compelling visual stories** that reflect the MCU's cinematic growth and interconnected storytelling.

---

### 👨‍💻 **My Role:**  
As the sole analyst and developer, I led the **end-to-end data pipeline** — from automated data retrieval and transformation to visualization and storytelling — showcasing full-stack data analysis skills and cloud integration.

---

### 🛠️ **Key Technical Contributions:**

- **Cloud-based Data Acquisition (AWS S3):**
  - Programmatically connected to **AWS S3** to retrieve multiple movie-related datasets, including:
    - Movie metadata (titles, release dates, durations)
    - Cast and character lists
    - Box office and rating statistics
  - Automated the download and parsing of CSV files using `boto3`, eliminating manual steps.

- **Python Data Processing:**
  - Cleaned and merged 3 distinct datasets using `pandas`, handling:
    - Date parsing (e.g., release years, timelines)
    - String normalization (e.g., character aliases, movie titles)
    - Missing value treatment for box office stats
  - Engineered new features such as:
    - Year-wise cumulative movie count
    - Top recurring characters and their screen appearances
    - Phase classification (MCU Phase 1–4)

- **Visualization & Storytelling:**
  - Created **7+ interactive and static charts** using `Matplotlib` and `Seaborn`:
    - 🎞️ Timeline of MCU movie releases by phase
    - 🎭 Heatmap of character appearances across movies
    - 📈 Line chart of box office growth over time
    - 🧑‍🤝‍🧑 Bar chart of most-featured characters
  - Focused on readability and insight-driven layout to support stakeholder storytelling.

---

### 📊 **Impact & Insights:**

- 📅 **Release Acceleration:**  
  Identified a clear trend of **increasing release frequency** post-2015, especially in **Phase 3**, signaling Marvel's strategy to deepen audience engagement and expand the cinematic universe faster.

- 🎭 **Character Popularity & Dynamics:**  
  Discovered that **Iron Man, Captain America, and Black Widow** were the most prominently featured characters, not only in terms of screen time but also in cross-over appearances — highlighting their roles as narrative anchors in the MCU.

- 💰 **Box Office Evolution:**  
  Analysis revealed a consistent **increase in box office revenue** with each phase, peaking during ensemble movies (e.g., *Avengers: Endgame*), proving the effectiveness of Marvel’s long-form storytelling and phased releases.

- 🧑‍🤝‍🧑 **Team-Based Character Clusters:**  
  Heatmaps of character appearances showed strong **group dynamics**, such as Avengers vs. Guardians, with occasional crossovers. These insights could inform **future marketing strategies or storytelling directions** for franchise expansions.

- 🧪 **Use Case for Data-Driven Storytelling:**  
  Demonstrated how a pop culture dataset can be transformed into **insightful narratives** using data science, emphasizing the power of combining **data engineering, visualization, and communication**.

---

## Project: 3 🧠 **Object Recognition | Swansea University**  
📅 **Duration:** July 2024 – September 2024  
🔧 **Tech Stack:** Python, TensorFlow, Keras, Scikit-learn, NumPy, Matplotlib, CIFAR-100

---

### **Project Overview:**  
This project focused on comparing multiple deep learning architectures—EfficientNet, DenseNet, and a traditional CNN—on the CIFAR-100 dataset to evaluate their performance in object classification. The goal was to identify the most efficient and accurate model, reduce classification errors, and enhance prediction reliability through ensemble methods and robust data engineering practices.

---

### **My Role:**  
I independently led the entire project pipeline, from dataset preparation and model implementation to evaluation and ensemble design. I also handled all data engineering tasks and generated visual insights to interpret model performance.

---

### **Key Technical Contributions:**

- **Model Implementation & Evaluation:**
  - Built and trained EfficientNet, DenseNet, and a baseline CNN model on the CIFAR-100 dataset.
  - Compared model performance using key evaluation metrics like accuracy, precision, recall, and F1-score.

- **Ensemble Learning:**
  - Applied **Majority Voting** and **Weighted Majority Voting** techniques to improve overall prediction accuracy.
  - Ensemble strategy helped reach over **80% accuracy**, significantly reducing false negatives.

- **Data Engineering & Preprocessing:**
  - Preprocessed 60,000+ image samples: resizing, normalization, label encoding, and augmentation.
  - Enhanced dataset quality by **20%** through noise reduction and class balancing, improving training efficiency.

- **Visualization & Reporting:**
  - Used `Matplotlib` and `Seaborn` to create training plots, confusion matrices, and per-class accuracy charts.
  - Interpreted model behavior and highlighted key trends across experiments.

---

### **Impact & Insights:**

- Ensemble methods improved classification reliability, especially for **visually similar classes**, reducing false negatives by **up to 19%**.
- EfficientNet achieved the highest single-model accuracy, while DenseNet showed better performance for minority and underrepresented classes.
- Data enhancements directly contributed to more stable training and clearer class boundaries, enabling deeper analysis and higher confidence in results.
- The project laid the foundation for scalable experimentation with CNNs and ensemble methods, applicable in real-world object recognition problems.

---

## Project: 4 ⚙️ **Optimisers | Swansea University**  
📅 **Date:** November 2023  
🔧 **Tech Stack:** Python, NumPy, Matplotlib, Random Search, Simulated Annealing

---

### **Project Overview:**  
This project focused on optimizing the design of **hydrostatic thrust bearings** used in high-precision mechanical systems. The aim was to minimize **power losses** while maintaining load-bearing capacity, using algorithmic approaches to search for optimal design parameters. By implementing heuristic-based optimisers in Python, the project showcased the real-world impact of **engineering-focused optimization algorithms**.

---

### **My Role:**  
I designed, developed, and evaluated two metaheuristic optimisation algorithms — **Random Search** and **Simulated Annealing** — to optimize a complex multi-variable function governing bearing efficiency. I also visualized convergence behaviors and interpreted performance improvements across algorithmic runs.

---

### **Key Technical Contributions:**

- **Algorithm Design & Implementation:**
  - Built custom Python implementations of **Random Search** and **Simulated Annealing** tailored to the problem of energy loss in hydrostatic bearings.
  - Encoded physical constraints of the bearing system (e.g., geometry, pressure, viscosity) directly into the cost function.

- **Objective Function & Constraints:**
  - Designed a multi-objective fitness function to simulate **power loss, load support, and oil flow rates**, allowing for accurate comparison of candidate solutions.
  - Applied boundary checks and rejection conditions to maintain valid design configurations.

- **Parameter Tuning & Testing:**
  - Conducted experiments with various cooling schedules and neighborhood sizes for Simulated Annealing to prevent premature convergence.
  - Compared performance across multiple runs and random seeds to validate robustness.

- **Visualization & Convergence Analysis:**
  - Generated convergence plots and parameter vs. performance graphs to visually demonstrate optimisation progress and final solution stability.

---

### **Impact & Insights:**

- Successfully reduced power loss in the bearing system by **53%**, demonstrating the effectiveness of heuristic search methods over manual tuning or grid search approaches.
- Simulated Annealing outperformed Random Search in finding globally optimal configurations within fewer iterations, highlighting its strength in **exploring rugged solution landscapes**.
- Reinforced the practical value of optimisation techniques in engineering domains, especially where analytical solutions are difficult or infeasible.
- The project served as a hands-on application of **AI-inspired algorithms** in mechanical design, bridging the gap between data science and engineering optimization.

---

## 🎓 Certifications
- **Introduction to Data Analytics** (Jul 2022)  
- **Python for Data Science, AI, and Development** (Aug 2022)  
- **Introduction to Project Management** (Aug,2022)

---

## 🔗 Contact & Social Links
- [LinkedIn](https://www.linkedin.com/in/vandit-bhut/){:target="_blank"}
- Email: vanditbhut12@gmail.com  
- Phone: +44 739-306-5231

---

## Looking Ahead
I am currently seeking a data science role where I can apply my technical expertise and analytical skills to address complex business challenges. I am excited to continue learning, growing, and contributing to data-driven projects that drive impactful results.

---

## 🚀 Get in Touch  
Feel free to reach out to me via my contact details above to discuss potential opportunities or collaborations. Thank you for visiting my portfolio!




